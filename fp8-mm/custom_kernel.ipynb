{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 假设 hip_kernel.so 是编译后的共享库\n",
    "hip_kernel = ctypes.cdll.LoadLibrary(\"./hip_kernel.so\")\n",
    "\n",
    "# 假设 kernel 函数名为 run_fp8_gemm，并且是用 extern \"C\" 暴露的\n",
    "hip_kernel.run_fp8_gemm.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_uint8),   # a: float8_e4m3fnuz\n",
    "    ctypes.POINTER(ctypes.c_uint8),   # b: float8_e4m3fnuz\n",
    "    ctypes.POINTER(ctypes.c_float),   # a_scale\n",
    "    ctypes.POINTER(ctypes.c_float),   # b_scale\n",
    "    ctypes.POINTER(ctypes.c_uint16),  # c: bf16\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int  # m, n, k\n",
    "]\n",
    "\n",
    "def custom_kernel(data: input_t) -> output_t:\n",
    "    a, b, a_scale, b_scale, c = data\n",
    "    m, k = a.shape\n",
    "    n = b.shape[0]\n",
    "\n",
    "    # 转换 float8 → uint8（将 view 为 uint8）\n",
    "    a_np = a.contiguous().view(torch.uint8).cpu().numpy()\n",
    "    b_np = b.contiguous().view(torch.uint8).cpu().numpy()\n",
    "\n",
    "    # scaling 因子为 float32\n",
    "    a_scale_np = a_scale.contiguous().cpu().numpy().astype(np.float32)\n",
    "    b_scale_np = b_scale.contiguous().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # bf16 → uint16（bf16 是 PyTorch 中 16bit）\n",
    "    c_np = c.contiguous().view(torch.uint16).cpu().numpy()\n",
    "\n",
    "    # 调用 HIP kernel\n",
    "    hip_kernel.run_fp8_gemm(\n",
    "        a_np.ctypes.data_as(ctypes.POINTER(ctypes.c_uint8)),\n",
    "        b_np.ctypes.data_as(ctypes.POINTER(ctypes.c_uint8)),\n",
    "        a_scale_np.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        b_scale_np.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        c_np.ctypes.data_as(ctypes.POINTER(ctypes.c_uint16)),\n",
    "        ctypes.c_int(m), ctypes.c_int(n), ctypes.c_int(k)\n",
    "    )\n",
    "\n",
    "    # 将结果写回 PyTorch Tensor（注意数据类型）\n",
    "    c.copy_(torch.from_numpy(c_np).view(torch.bfloat16).to(\"cuda\"))\n",
    "\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qin/.cache/torch_extensions/py312_cpu/fp8_mm/main.cpp -> /home/qin/.cache/torch_extensions/py312_cpu/fp8_mm/main.cpp [skipped, no changes]\n",
      "/home/qin/.cache/torch_extensions/py312_cpu/fp8_mm/cuda.cu -> /home/qin/.cache/torch_extensions/py312_cpu/fp8_mm/hip.hip [skipped, already hipified]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/qin/.cache/torch_extensions/py312_cpu as PyTorch extensions root...\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 1\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/opt/rocm-6.3.3/bin/hipcc', '-v']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m\n\u001b[1;32m     61\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHSA_OVERRIDE_GFX_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m11.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# os.environ[\"RANK\"] = \"0\"          # 单GPU需设置 RANK=0\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# os.environ[\"WORLD_SIZE\"] = \"1\"    # 总进程数为1\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# os.environ[\"MASTER_ADDR\"] = \"localhost\"  # 设置主节点地址\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# os.environ[\"MASTER_PORT\"] = \"12355\"      # 设置通信端口\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m module \u001b[38;5;241m=\u001b[39m load_inline(\n\u001b[1;32m     69\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp8_mm\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     70\u001b[0m     cpp_sources\u001b[38;5;241m=\u001b[39m[CPP_WRAPPER],\n\u001b[1;32m     71\u001b[0m     cuda_sources\u001b[38;5;241m=\u001b[39m[CUDA_SRC],\n\u001b[1;32m     72\u001b[0m     functions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp8_mm\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     73\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m     extra_cuda_cflags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--offload-arch=gfx1100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-std=c++20\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_kernel\u001b[39m(data: input_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m output_t:\n\u001b[1;32m     79\u001b[0m     a, b, a_scale, b_scale, c \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/anaconda3/envs/Llama/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1723\u001b[0m, in \u001b[0;36mload_inline\u001b[0;34m(name, cpp_sources, cuda_sources, functions, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, with_pytorch_error_handling, keep_intermediates, use_pch)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     _maybe_write(cuda_source_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cuda_sources))\n\u001b[1;32m   1721\u001b[0m     sources\u001b[38;5;241m.\u001b[39mappend(cuda_source_path)\n\u001b[0;32m-> 1723\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _jit_compile(\n\u001b[1;32m   1724\u001b[0m     name,\n\u001b[1;32m   1725\u001b[0m     sources,\n\u001b[1;32m   1726\u001b[0m     extra_cflags,\n\u001b[1;32m   1727\u001b[0m     extra_cuda_cflags,\n\u001b[1;32m   1728\u001b[0m     extra_ldflags,\n\u001b[1;32m   1729\u001b[0m     extra_include_paths,\n\u001b[1;32m   1730\u001b[0m     build_directory,\n\u001b[1;32m   1731\u001b[0m     verbose,\n\u001b[1;32m   1732\u001b[0m     with_cuda,\n\u001b[1;32m   1733\u001b[0m     is_python_module,\n\u001b[1;32m   1734\u001b[0m     is_standalone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1735\u001b[0m     keep_intermediates\u001b[38;5;241m=\u001b[39mkeep_intermediates)\n",
      "File \u001b[0;32m~/anaconda3/envs/Llama/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1798\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 hipified_sources\u001b[38;5;241m.\u001b[39madd(hipify_result[s_abs]\u001b[38;5;241m.\u001b[39mhipified_path \u001b[38;5;28;01mif\u001b[39;00m s_abs \u001b[38;5;129;01min\u001b[39;00m hipify_result \u001b[38;5;28;01melse\u001b[39;00m s_abs)\n\u001b[1;32m   1796\u001b[0m             sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(hipified_sources)\n\u001b[0;32m-> 1798\u001b[0m         _write_ninja_file_and_build_library(\n\u001b[1;32m   1799\u001b[0m             name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1800\u001b[0m             sources\u001b[38;5;241m=\u001b[39msources,\n\u001b[1;32m   1801\u001b[0m             extra_cflags\u001b[38;5;241m=\u001b[39mextra_cflags \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[1;32m   1802\u001b[0m             extra_cuda_cflags\u001b[38;5;241m=\u001b[39mextra_cuda_cflags \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[1;32m   1803\u001b[0m             extra_ldflags\u001b[38;5;241m=\u001b[39mextra_ldflags \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[1;32m   1804\u001b[0m             extra_include_paths\u001b[38;5;241m=\u001b[39mextra_include_paths \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[1;32m   1805\u001b[0m             build_directory\u001b[38;5;241m=\u001b[39mbuild_directory,\n\u001b[1;32m   1806\u001b[0m             verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1807\u001b[0m             with_cuda\u001b[38;5;241m=\u001b[39mwith_cuda,\n\u001b[1;32m   1808\u001b[0m             is_standalone\u001b[38;5;241m=\u001b[39mis_standalone)\n\u001b[1;32m   1809\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m verbose:\n\u001b[1;32m   1810\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo modifications detected for re-loaded extension \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1811\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipping build step...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[0;32m~/anaconda3/envs/Llama/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1892\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1888\u001b[0m verify_ninja_availability()\n\u001b[1;32m   1890\u001b[0m compiler \u001b[38;5;241m=\u001b[39m get_cxx_compiler()\n\u001b[0;32m-> 1892\u001b[0m get_compiler_abi_compatibility_and_version(compiler)\n\u001b[1;32m   1893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_cuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1894\u001b[0m     with_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_is_cuda_file, sources))\n",
      "File \u001b[0;32m~/anaconda3/envs/Llama/lib/python3.12/site-packages/torch/utils/cpp_extension.py:392\u001b[0m, in \u001b[0;36mget_compiler_abi_compatibility_and_version\u001b[0;34m(compiler)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, TorchVersion(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# First check if the compiler is one of the expected ones for the particular platform.\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_compiler_ok_for_platform(compiler):\n\u001b[1;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(WRONG_COMPILER_WARNING\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    394\u001b[0m         user_compiler\u001b[38;5;241m=\u001b[39mcompiler,\n\u001b[1;32m    395\u001b[0m         pytorch_compiler\u001b[38;5;241m=\u001b[39m_accepted_compilers_for_platform()[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    396\u001b[0m         platform\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mplatform))\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, TorchVersion(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/Llama/lib/python3.12/site-packages/torch/utils/cpp_extension.py:354\u001b[0m, in \u001b[0;36mcheck_compiler_ok_for_platform\u001b[0;34m(compiler)\u001b[0m\n\u001b[1;32m    352\u001b[0m env \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    353\u001b[0m env[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLC_ALL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Don't localize output\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m version_string \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output([compiler, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-v\u001b[39m\u001b[38;5;124m'\u001b[39m], stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT, env\u001b[38;5;241m=\u001b[39menv)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;241m*\u001b[39mSUBPROCESS_DECODE_ARGS)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IS_LINUX:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Check for 'gcc' or 'g++' for sccache wrapper\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^COLLECT_GCC=(.*)$\u001b[39m\u001b[38;5;124m\"\u001b[39m, re\u001b[38;5;241m.\u001b[39mMULTILINE)\n",
      "File \u001b[0;32m~/anaconda3/envs/Llama/lib/python3.12/subprocess.py:468\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         empty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    466\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run(\u001b[38;5;241m*\u001b[39mpopenargs, stdout\u001b[38;5;241m=\u001b[39mPIPE, timeout\u001b[38;5;241m=\u001b[39mtimeout, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    469\u001b[0m            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[0;32m~/anaconda3/envs/Llama/lib/python3.12/subprocess.py:573\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    574\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/opt/rocm-6.3.3/bin/hipcc', '-v']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# This script provides a template for using load_inline to run a HIP kernel for\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from task import input_t, output_t\n",
    "CPP_WRAPPER = \"\"\"\n",
    "void fp8_mm(torch::Tensor a, torch::Tensor b, torch::Tensor as, torch::Tensor bs, torch::Tensor c);\n",
    "\"\"\"\n",
    "\n",
    "CUDA_SRC = \"\"\"\n",
    "#include <hip/amd_detail/amd_hip_fp8.h>\n",
    "#include <hip/amd_detail/amd_hip_bf16.h>\n",
    "\n",
    "constexpr const int BLOCK = 128;\n",
    "\n",
    "__global__ void custom_kernel(const __hip_fp8_e4m3_fnuz* a, const __hip_fp8_e4m3_fnuz* b, const float* as, const float* bs, \n",
    "                   __hip_bfloat16* c, int m, int n, int k) {\n",
    "                   \n",
    "    // Your implementation here\n",
    "    int cx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    int cy = threadIdx.y + blockDim.y * blockIdx.y;\n",
    "    if(cx >= m || cy >= n) return;\n",
    "    \n",
    "    int sn = (n + BLOCK - 1) / BLOCK;\n",
    "    \n",
    "    float result = 0;\n",
    "    // split loop into an outer loop over different blocks, and an inner loop within one block.\n",
    "    // we can assume k % BLOCK == 0.\n",
    "    for(int i = 0; i < k; i += BLOCK) {\n",
    "        // block results accumulates the inner product across a single block.\n",
    "        // within each block, scales are constant, so we can lift the scaling \n",
    "        // outside of the inner loop.\n",
    "        float block_result = 0;\n",
    "        for(int ii = 0; ii < BLOCK; ++ii) {\n",
    "            // load input matrix elements and convert to float for computations\n",
    "            float av = (float)a[cx + (i + ii) * m];\n",
    "            float bv = (float)b[cy + (i + ii) * n];\n",
    "            block_result += av * bv; \n",
    "        }\n",
    "        \n",
    "        // before we can go to the next block, scale the result of the current block\n",
    "        // and accumulate to final result\n",
    "        // note the different indexing into as and bs\n",
    "        result += block_result * as[cx + i/BLOCK * m] * bs[cy/BLOCK + i/BLOCK * sn];\n",
    "    }\n",
    "    \n",
    "    // finally, write the result as bf16\n",
    "    c[cx * n + cy] = (__hip_bfloat16)result;\n",
    "}\n",
    "\n",
    "void fp8_mm(torch::Tensor a, torch::Tensor b, torch::Tensor as, torch::Tensor bs, torch::Tensor c) {\n",
    "    int m = a.size(0);\n",
    "    int n = b.size(0);\n",
    "    int k = a.size(1);\n",
    "    custom_kernel<<<dim3((m+15)/16, (n+15)/16), dim3(16, 16), 0, 0>>> ((__hip_fp8_e4m3_fnuz*)a.data_ptr(), (__hip_fp8_e4m3_fnuz*)b.data_ptr(), \n",
    "    as.data_ptr<float>(), bs.data_ptr<float>(), (__hip_bfloat16*)c.data_ptr(), m, n, k);\n",
    "    //C10_CUDA_CHECK(cudaGetLastError());\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CXX\"] = \"/opt/rocm-6.3.3/bin/hipcc\"\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "\n",
    "# os.environ[\"RANK\"] = \"0\"          # 单GPU需设置 RANK=0\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\"    # 总进程数为1\n",
    "# os.environ[\"MASTER_ADDR\"] = \"localhost\"  # 设置主节点地址\n",
    "# os.environ[\"MASTER_PORT\"] = \"12355\"      # 设置通信端口\n",
    "\n",
    "module = load_inline(\n",
    "    name='fp8_mm',\n",
    "    cpp_sources=[CPP_WRAPPER],\n",
    "    cuda_sources=[CUDA_SRC],\n",
    "    functions=['fp8_mm'],\n",
    "    verbose=True,\n",
    "    extra_cuda_cflags=[\"--offload-arch=gfx1100\", \"-std=c++20\"],\n",
    ")\n",
    "\n",
    "\n",
    "def custom_kernel(data: input_t) -> output_t:\n",
    "    a, b, a_scale, b_scale, c = data\n",
    "    module.fp8_mm(a, b, a_scale, b_scale, c)\n",
    "    return c\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
